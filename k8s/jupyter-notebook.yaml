apiVersion: v1
kind: Pod
metadata:
  name: vlm-jupyter-debug
  labels:
    app: mdlm-code-fixer
    task: debugger
spec:
  restartPolicy: Never
  containers:
  - name: jupyter-container
    image: pytorch/pytorch:2.5.1-cuda12.4-cudnn9-devel
    command:
      - bash
      - -c
      - |
        apt-get update && apt-get install -y git &&
        pip install --no-cache-dir jupyterlab "transformers==4.44.2" peft accelerate bitsandbytes &&
        git clone https://github.com/bhavyagupta98/mdlm-bug-fixer.git /workspace/mdlm-bug-fixer &&
        jupyter lab --ip=0.0.0.0 --no-browser --allow-root --notebook-dir=/workspace/mdlm-bug-fixer --NotebookApp.token=selfimprove

    resources:
      limits:
        nvidia.com/rtxa6000: "1"
        memory: "32Gi"
        cpu: "2"
      requests:
        nvidia.com/rtxa6000: "1"
        memory: "32Gi"
        cpu: "2"

    volumeMounts:
      - name: cache-volume
        mountPath: /cache
      - name: outputs-volume
        mountPath: /outputs
      - name: dev-shm
        mountPath: /dev/shm

    env:
    # HuggingFace token (required for reward model)
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token
          key: token
          optional: true

    # Cache directories
    - name: HF_HOME
      value: "/cache/hf"
    - name: TRANSFORMERS_CACHE
      value: "/cache/hf/transformers"
    - name: HF_DATASETS_CACHE
      value: "/cache/hf/datasets"
    - name: TORCH_HOME
      value: "/cache/torch"
    - name: PIP_CACHE_DIR
      value: "/cache/pip"
    - name: OUTPUT_DIR
      value: "/outputs"

    # Python and CUDA settings
    - name: PYTHONUNBUFFERED
      value: "1"
    - name: PYTORCH_CUDA_ALLOC_CONF
      value: "expandable_segments:True"

  tolerations:
    - key: nautilus.io/reservation
      operator: Exists
      effect: NoSchedule
    - effect: NoSchedule
      key: nautilus.io/hardware
      operator: Equal
      value: large-gpu
    - effect: PreferNoSchedule
      key: nvidia.com/gpu
      operator: Exists
    # Tolerate nautilus.io/issue taints (nodes with known issues but still usable)
    # - key: nautilus.io/issue
    #   operator: Exists
    #   effect: NoSchedule

  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: nvidia.com/gpu.product
  #           operator: In
  #           values:
  #           - NVIDIA-A100-SXM4-80GB

  volumes:
    - name: cache-volume
      emptyDir:
        sizeLimit: 100Gi
    - name: outputs-volume
      persistentVolumeClaim:
        claimName: mdlm-outputs
    - name: dev-shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
